{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003a9cd1",
   "metadata": {},
   "source": [
    "# Incluindo validação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as transF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a3875-c040-4da5-a76f-a6390b98db77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_aug = iaa.Sequential([\n",
    "            iaa.Sometimes(0.25, iaa.Affine(scale={\"x\": (1.0, 2.0), \"y\": (1.0, 2.0)})),\n",
    "            iaa.Resize((224, 224)),\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Flipud(0.2),\n",
    "            iaa.Sometimes(0.25, iaa.Affine(rotate=(-120, 120), mode='symmetric')),\n",
    "            iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "\n",
    "            # noise\n",
    "            iaa.Sometimes(0.1,\n",
    "                          iaa.OneOf([\n",
    "                              iaa.Dropout(p=(0, 0.05)),\n",
    "                              iaa.CoarseDropout(0.02, size_percent=0.25)\n",
    "                          ])),\n",
    "\n",
    "            iaa.Sometimes(0.25,\n",
    "                          iaa.OneOf([\n",
    "                              iaa.Add((-15, 15), per_channel=0.5), # brightness\n",
    "                              iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "                          ])),\n",
    "\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25215f0f-21ec-465b-ac47-5b67ecd2b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trans = transforms.Compose([\n",
    "                np.array,\n",
    "                my_aug.augment_image,\n",
    "                np.copy,\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfc352-18bc-4899-93fc-1261c333ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trans_eval = transforms.Compose([             \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce692e0d-f040-4133-8a16-d69975b6b1d4",
   "metadata": {},
   "source": [
    "## Vamos usar o transform com o `my_dataset`\n",
    "- Vamos usar os resultados do notebook anterior para carregar os dados da base de felinos novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8783f-a814-47e9-8066-78b3f836c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset (torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Esse dataset recebe uma lista de path de imagens, uma lista de labels correspondentes e (opcional) transformações pata aplicar nas imagens\n",
    "    quando elas forem carregadas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imgs_path, labels, my_transform=None):\n",
    "        \"\"\"\n",
    "        imgs_path: list ou tuple\n",
    "            Uma lista ou tupla com os paths para todas as imagens\n",
    "        labels: lista ou tuple\n",
    "            Uma lista ou tupla com o label de todas as imagens. Obviamente, precisa dar match com o os paths\n",
    "        my_transform: None ou torchvision.transforms\n",
    "            Uma sequência de transformadores para aplicar nos dados. Se for None, ele apenas transforma em tensor\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.imgs_path = imgs_path\n",
    "        self.labels = labels\n",
    "        \n",
    "        # se my_transform for None, precisamos garantir que a imagem PIL seja transformada em Tensor para nao \n",
    "        # obtermos um erro quando usarmos o dataloader (ver aulas passadas)        \n",
    "        if my_transform is not None:\n",
    "            self.transform = my_transform\n",
    "        else:\n",
    "            self.transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "    def __len__(self):     \n",
    "        \"\"\"\n",
    "        Sobrecarga do método len para obtermos o tamanho do dataset. Não é obrigatório implementar\n",
    "        \"\"\"\n",
    "        return len(self.imgs_path)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):        \n",
    "        \"\"\"\n",
    "        Esse método obtém uma imagem e um label cada vez que iteramos no Dataset. Ele também aplica a transformação\n",
    "        na imagem. É obrigatório sua implementação\n",
    "        \n",
    "        item: int \n",
    "            Um indice no intervalo [0, ..., len(img_paths)-1]\n",
    "        \n",
    "        return: tuple \n",
    "             Uma tupla com a imagem, label e ID da imagem correspondentes ao item\n",
    "        \"\"\"\n",
    "\n",
    "        # Aqui usamos PIL para carregar as imagens\n",
    "        image = Image.open(self.imgs_path[item]).convert(\"RGB\")\n",
    "\n",
    "        # Aplicando as transformações\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Obtendo o ID da imagem\n",
    "        img_id = self.imgs_path[item].split('/')[-1].split('.')[0]\n",
    "\n",
    "        if self.labels is None:\n",
    "            labels = []\n",
    "        else:\n",
    "            labels = self.labels[item]\n",
    "\n",
    "        return image, labels, img_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4607d58-02c8-4f19-8bc0-d53076c79561",
   "metadata": {},
   "source": [
    "#### Preparando dados para criarmos um dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3105b12-b75e-4f00-acad-8aed810dbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b86514-d799-435d-8ce5-57c22f0b0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/home/patcha/datasets/felinos/train\"\n",
    "test_data_path = \"/home/patcha/datasets/felinos/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81ab73-a59b-4501-804a-92e213fdfd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name = glob(os.path.join(train_data_path, \"*\"))\n",
    "labels_name = [l.split(os.path.sep)[-1] for l in labels_name]\n",
    "labels_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224c516-e4f4-436f-bcd6-8289479a0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_and_labels(path, lab_names):\n",
    "    imgs_path, labels = list(), list()\n",
    "    lab_cnt = 0\n",
    "    for lab in lab_names:    \n",
    "        paths_aux = glob(os.path.join(path, lab, \"*.jpg\"))\n",
    "        \n",
    "        # Atualizando os labels e imgs_paths\n",
    "        labels += [lab_cnt] * len(paths_aux)\n",
    "        imgs_path += paths_aux\n",
    "        \n",
    "        lab_cnt += 1\n",
    "        \n",
    "    return imgs_path, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8294d-5028-46be-b7d8-6ca82e84856d",
   "metadata": {},
   "source": [
    "- Obtendo paths e labels para cada partição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9baefd6-ab66-4866-b313-00d6d2dc1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_paths, train_labels = get_paths_and_labels(train_data_path, labels_name)\n",
    "temp_imgs_paths, temp_labels = get_paths_and_labels(test_data_path, labels_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91854e0b-c936-4166-bf29-c5094262a52e",
   "metadata": {},
   "source": [
    "- **Novo**: criando o conjunto de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4b343-82d2-4b14-bf76-b40934e9256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_imgs_paths, val_imgs_paths, test_labels, val_labels = train_test_split(temp_imgs_paths, temp_labels, test_size=0.5, random_state=10)\n",
    "len(test_imgs_paths), len(val_imgs_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5ae47-557d-43e8-9209-0cbd201c5f88",
   "metadata": {},
   "source": [
    "#### Instanciando os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32175df5-b75f-4228-bcab-98977995b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_imgs_paths, train_labels, my_transform=my_trans)\n",
    "test_dataset = MyDataset(test_imgs_paths, test_labels, my_transform=my_trans_eval)\n",
    "val_dataset = MyDataset(val_imgs_paths, val_labels, my_transform=my_trans_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf209b-abca-4f9c-a3dd-66f543aa4854",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Criando um Dataloader\n",
    "- Agora, podemos criar um [dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) como fizemos nas últimas duas aulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213164a5-937b-446a-ad2a-dadf4ee81f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa362d3",
   "metadata": {},
   "source": [
    "## (Re)criando uma CNN\n",
    "- Vamos criar uma CNN para classificar os felinos\n",
    "- Para facilitar o projeto, vamos criar uma função que calcula o tamanho da saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=2), # 109\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # 54\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=7, stride=2), # 24\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=4)) # 6\n",
    "        self.fc = nn.Linear(576, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)        \n",
    "        out = self.layer2(out)\n",
    "        # Fazendo a operação de flatten        \n",
    "        out = out.reshape(out.size(0), -1)        \n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7bdf2",
   "metadata": {},
   "source": [
    "- Agora podemos instanciar o nosso modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc94922",
   "metadata": {},
   "source": [
    "- Agora podemos determinar nossa função de custo e otimizador\n",
    "- Para esse notebook vamos aproveitar o que já fizemos no anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a2ad1",
   "metadata": {},
   "source": [
    "- Agora vamos fazer nosso loop de treinamento\n",
    "- Agora, vamos mandar nosso modelo para GPU se ela estiver disponível\n",
    "- **Novo**: vamos criar nosso pipeline de checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de13ef-eb70-4eca-9f60-2a6dbb54e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(data_loader):    \n",
    "    with torch.no_grad():\n",
    "        loss, cnt = 0, 0\n",
    "        for images, labels, img_id in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss += loss_func(outputs, labels).item()\n",
    "            cnt += 1\n",
    "    return loss/cnt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33283d-ee39-453f-a500-99412a8e8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Movendo o modelo para o device alvo\n",
    "model.to(device)\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    loss_epoch, cnt = 0, 0\n",
    "    for k, (batch_images, batch_labels, id_img) in enumerate(train_dataloader):  \n",
    "        \n",
    "        # Aplicando um flatten na imagem e movendo ela para o device alvo\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Fazendo a forward pass\n",
    "        # observe que o modelo é agnóstico ao batch size\n",
    "        outputs = model(batch_images)\n",
    "        loss = loss_func(outputs, batch_labels)\n",
    "        loss_epoch += loss.item()\n",
    "        \n",
    "        # Fazendo a otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        cnt += 1\n",
    "        \n",
    "        \n",
    "    loss_epoch = loss_epoch / cnt\n",
    "    loss_val_epoch = eval_loss(val_dataloader)\n",
    "    \n",
    "    \n",
    "    writer.add_scalar(\"LossTrain\", loss_epoch, epoch)\n",
    "    writer.add_scalar(\"LossVal\", loss_val_epoch, epoch)\n",
    "    \n",
    "    temp = {\n",
    "        \"Train\": loss_epoch,\n",
    "        \"Val\": loss_val_epoch\n",
    "    }\n",
    "    writer.add_scalars(\"Loss\", temp, epoch)\n",
    "    \n",
    "        \n",
    "    # Salvando o checkpoint da última época\n",
    "    checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_func,\n",
    "            'loss_val': loss_epoch\n",
    "    }        \n",
    "    torch.save(checkpoint, \"last_checkpoint.pth\")\n",
    "\n",
    "    # Salvando a mellhor execução    \n",
    "    if loss < best_loss:        \n",
    "        best_loss = loss\n",
    "        torch.save(checkpoint, \"best_checkpoint.pth\")\n",
    "        \n",
    "    \n",
    "    print (f\"- Epoch [{epoch+1}/{num_epochs}] | Loss: {loss_epoch:.4f} | Loss Val: {loss_val_epoch:.4f}\")                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf3356",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fazendo inferência no conjunto de teste\n",
    "- A inferência é basicamente igual a do notebook anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f3de3-e1f8-4bad-91c5-792ade70b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdaec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for images, labels, img_id in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac922d-21eb-4df7-822b-1c6093f82982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
