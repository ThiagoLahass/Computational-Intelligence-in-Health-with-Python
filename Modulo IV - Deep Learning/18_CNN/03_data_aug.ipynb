{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003a9cd1",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "- O objetivo deste notebook é explorarmos um pouco sobre data augmentation\n",
    "- Para isso, vamos explorar os seguintes módulos:\n",
    "    1. [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)\n",
    "    2. [imgaug](https://github.com/aleju/imgaug)\n",
    "        - **Importante:** a biblioteca ficou depreciada e parou de ser mantida. Aqui existe um fork atualizado: https://github.com/life-ufes/imgaug-life\n",
    "    \n",
    "- Também vamos falar um pouco sobre os modulos `Functional` do Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as transF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132c1e8-5e17-4d84-8426-055e5b683628",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Usando o `torchvision.transforms`\n",
    "- Esse é o módulo oficial do Pytorch para aplicarmos transformações e augmentation em imagens\n",
    "- As transformações podem ser encadeadas (usando `Compose`) e a maioria das classes tem equivalente na versão `functional`\n",
    "- A maioria das transformações aceitam PIL images e tensores\n",
    "    - Algumas são apenas para PIL e outras apenas para tensores\n",
    "    - Tem que consultar a documentação quando for usar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6feb4-2e80-4af6-9102-f83f7d216860",
   "metadata": {},
   "source": [
    "#### Vamos carregar uma imagem com PIL para aplicarmos algumas transformações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148cd60-6b8e-483b-8c3a-f2fcf9f46eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"101.jpg\")\n",
    "image # como é um PIL, o jupyter ja plota como imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d4075-8924-40c0-b6b6-77608b7576bd",
   "metadata": {},
   "source": [
    "- Vamos aplicar um resize usando o `transforms`\n",
    "    - https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178606fb-14f8-421f-80f5-86d50442eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_op = transforms.Resize((120, 120))\n",
    "res_op(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674338a-fc40-4283-bee2-b63e5989846c",
   "metadata": {},
   "source": [
    "- Perceba que a gente instanciou um objeto da classe resize e depois chamou ele\n",
    "- Isso é o padrão para criarmos composições de operações. Porém, para fazermos testes rápidos ou fora de composições, podemos usar o functional\n",
    "    - Nesse caso, vamos usar uma função ao invés de uma classe\n",
    "    - Como disse antes, quase todas as classes tem uma função correspondente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613ba8d-bdb7-4f19-8529-b86dab20a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "transF.resize(image, (120, 120))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df052413-ede7-4c8e-a4ff-82ff5748bec4",
   "metadata": {},
   "source": [
    "- Aplicando outras operações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54919c-5625-41c2-a3fa-21bc5f5052f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transF.vflip(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d993c90-f290-4c0b-819e-8d074f99c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "transF.adjust_brightness(image, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ae979-41d4-4ab7-8a9d-c42f18baf7f6",
   "metadata": {},
   "source": [
    "- E se quiseremos aplicar varias transformações?\n",
    "    - Uma opção seria ficar pegando o resultado da operação anterior e aplicar na próxima\n",
    "    - Porém, é mais fácil utilizar um `Compose`\n",
    "        - E para isso, temos que usar as classes\n",
    "- Também vamos adicionar aleatoriedade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be03d2-16bb-4bef-bb72-740e4c18342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_trans = transforms.Compose([\n",
    "                transforms.Resize((120, 120)),\n",
    "                transforms.RandomVerticalFlip(p=0.6),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.3)\n",
    "            ])\n",
    "\n",
    "many_trans(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1dcb4-7071-478e-b604-1e69e7ce047b",
   "metadata": {},
   "source": [
    "- Após aplicar os processamentos desejados, o padrão é transformar a imagem para tensor e também podemos aplicar uma normalização\n",
    "- Essa normalização pode ser entre 0 e 1\n",
    "- Porém, como muitas das vezes usamos um modelo pré-treinado na imageNet, usamos a mesma normalização que é usada la\n",
    "    - No caso, é uma normalização de média zero e desvio padrao 1 (Z-norm)\n",
    "    - Os valores são: média: `[0.485, 0.456, 0.406]` e desvio: `[0.229, 0.224, 0.225]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb9ae9-bece-4b5c-817f-c087de3fcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_trans = transforms.Compose([\n",
    "                transforms.Resize((120, 120)),\n",
    "                transforms.RandomVerticalFlip(p=0.6),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "many_trans(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d9374-fc60-46e6-b27b-1200de608d57",
   "metadata": {},
   "source": [
    "- **Importante**: se você possui um batch de images (ex: `[10, 3, 120, 120]`), ao chamar as operações dentro do `Compose`, ele aplica para todas as imagens no batch de maneira individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e77a2a-a7e6-4631-a27d-53c58ccb4b42",
   "metadata": {},
   "source": [
    "## Imgaug\n",
    "- Usar transforms é interessante e pode resolver a maioria dos casos\n",
    "- Porém, existem outras bibliotecas de augmentation que fornecem muito mais operações\n",
    "- Uma delas é a [imgaug](https://github.com/aleju/imgaug)\n",
    "- A ideia é a mesma, só muda um pouquinho os comandos e temos acesso mais transformações transformações para aplicarmos em uma imagem\n",
    "- E a lib usa OpenCV por trás dos panos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38196c5-be91-454c-a787-9d25fa19dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a3875-c040-4da5-a76f-a6390b98db77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_aug = iaa.Sequential([\n",
    "            iaa.Sometimes(0.25, iaa.Affine(scale={\"x\": (1.0, 2.0), \"y\": (1.0, 2.0)})),\n",
    "            iaa.Resize((224, 224)),\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Flipud(0.2),\n",
    "            iaa.Sometimes(0.25, iaa.Affine(rotate=(-120, 120), mode='symmetric')),\n",
    "            iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "\n",
    "            # noise\n",
    "            iaa.Sometimes(0.1,\n",
    "                          iaa.OneOf([\n",
    "                              iaa.Dropout(p=(0, 0.05)),\n",
    "                              iaa.CoarseDropout(0.02, size_percent=0.25)\n",
    "                          ])),\n",
    "\n",
    "            iaa.Sometimes(0.25,\n",
    "                          iaa.OneOf([\n",
    "                              iaa.Add((-15, 15), per_channel=0.5), # brightness\n",
    "                              iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "                          ])),\n",
    "\n",
    "        ])\n",
    "\n",
    "# Atenção aqui que se for mais de uma imagem (no caso de batch de imagens por exemplo), precisamos passar\n",
    "# images= ao invés de image=\n",
    "res = my_aug(image=np.asarray(image))\n",
    "plt.imshow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6d98d-9521-469d-af94-2450b3cd2fa8",
   "metadata": {},
   "source": [
    "- Podemos usar o método `show_grid()` pra gerar varias instancias do augmentation pra gente ter uma noção do que está acontencendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59324e24-8b90-46c2-a0a7-82e43fc55a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_aug.show_grid([np.asarray(image)], cols=4, rows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893911e-0f12-4cf5-a8a1-f87a427c50a0",
   "metadata": {},
   "source": [
    "- **Importante**: as operações da `imgaug` são feitas em cima de numpy arrays. Precisamos converter pra tensor e fazer a normalização no final dela\n",
    "- Para isso, podemos usar o `torchvision.transform`\n",
    "- Na verdade, podemos incluir a sequência do `imgaug` dentro do `compose` usando o método `augment_image`\n",
    "    - Só ter atenção para passarmos um Numpy array como entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffdca5-f533-4fd8-9d85-bc217b777c17",
   "metadata": {},
   "source": [
    "#### Juntando `imgaug` e `torchvision.transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25215f0f-21ec-465b-ac47-5b67ecd2b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trans = transforms.Compose([\n",
    "                np.array,\n",
    "                my_aug.augment_image,\n",
    "                np.copy,\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "res = my_trans(image)\n",
    "plt.imshow(res.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c7d69-5e6a-4e51-9a96-2b4e04715603",
   "metadata": {},
   "source": [
    "- Como o augmentation é só durante o treino, precisamos ter um transformador apenas para dados que vão ser avaliados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfc352-18bc-4899-93fc-1261c333ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trans_eval = transforms.Compose([             \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce692e0d-f040-4133-8a16-d69975b6b1d4",
   "metadata": {},
   "source": [
    "## Vamos usar o transform com o `my_dataset`\n",
    "- Vamos usar os resultados do notebook anterior para carregar os dados da base de felinos novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8783f-a814-47e9-8066-78b3f836c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset (torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Esse dataset recebe uma lista de path de imagens, uma lista de labels correspondentes e (opcional) transformações pata aplicar nas imagens\n",
    "    quando elas forem carregadas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imgs_path, labels, my_transform=None):\n",
    "        \"\"\"\n",
    "        imgs_path: list ou tuple\n",
    "            Uma lista ou tupla com os paths para todas as imagens\n",
    "        labels: lista ou tuple\n",
    "            Uma lista ou tupla com o label de todas as imagens. Obviamente, precisa dar match com o os paths\n",
    "        my_transform: None ou torchvision.transforms\n",
    "            Uma sequência de transformadores para aplicar nos dados. Se for None, ele apenas transforma em tensor\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.imgs_path = imgs_path\n",
    "        self.labels = labels\n",
    "        \n",
    "        # se my_transform for None, precisamos garantir que a imagem PIL seja transformada em Tensor para nao \n",
    "        # obtermos um erro quando usarmos o dataloader (ver aulas passadas)        \n",
    "        if my_transform is not None:\n",
    "            self.transform = my_transform\n",
    "        else:\n",
    "            self.transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "    def __len__(self):     \n",
    "        \"\"\"\n",
    "        Sobrecarga do método len para obtermos o tamanho do dataset. Não é obrigatório implementar\n",
    "        \"\"\"\n",
    "        return len(self.imgs_path)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):        \n",
    "        \"\"\"\n",
    "        Esse método obtém uma imagem e um label cada vez que iteramos no Dataset. Ele também aplica a transformação\n",
    "        na imagem. É obrigatório sua implementação\n",
    "        \n",
    "        item: int \n",
    "            Um indice no intervalo [0, ..., len(img_paths)-1]\n",
    "        \n",
    "        return: tuple \n",
    "             Uma tupla com a imagem, label e ID da imagem correspondentes ao item\n",
    "        \"\"\"\n",
    "\n",
    "        # Aqui usamos PIL para carregar as imagens\n",
    "        image = Image.open(self.imgs_path[item]).convert(\"RGB\")\n",
    "\n",
    "        # Aplicando as transformações\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Obtendo o ID da imagem\n",
    "        img_id = self.imgs_path[item].split('/')[-1].split('.')[0]\n",
    "\n",
    "        if self.labels is None:\n",
    "            labels = []\n",
    "        else:\n",
    "            labels = self.labels[item]\n",
    "\n",
    "        return image, labels, img_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4607d58-02c8-4f19-8bc0-d53076c79561",
   "metadata": {},
   "source": [
    "#### Preparando dados para criarmos um dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3105b12-b75e-4f00-acad-8aed810dbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b86514-d799-435d-8ce5-57c22f0b0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/home/patcha/datasets/felinos/train\"\n",
    "test_data_path = \"/home/patcha/datasets/felinos/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81ab73-a59b-4501-804a-92e213fdfd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name = glob(os.path.join(train_data_path, \"*\"))\n",
    "labels_name = [l.split(os.path.sep)[-1] for l in labels_name]\n",
    "labels_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224c516-e4f4-436f-bcd6-8289479a0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_and_labels(path, lab_names):\n",
    "    imgs_path, labels = list(), list()\n",
    "    lab_cnt = 0\n",
    "    for lab in lab_names:    \n",
    "        paths_aux = glob(os.path.join(path, lab, \"*.jpg\"))\n",
    "        \n",
    "        # Atualizando os labels e imgs_paths\n",
    "        labels += [lab_cnt] * len(paths_aux)\n",
    "        imgs_path += paths_aux\n",
    "        \n",
    "        lab_cnt += 1\n",
    "        \n",
    "    return imgs_path, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8294d-5028-46be-b7d8-6ca82e84856d",
   "metadata": {},
   "source": [
    "- Obtendo paths e labels para cada partição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9baefd6-ab66-4866-b313-00d6d2dc1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_paths, train_labels = get_paths_and_labels(train_data_path, labels_name)\n",
    "test_imgs_paths, test_labels = get_paths_and_labels(test_data_path, labels_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5ae47-557d-43e8-9209-0cbd201c5f88",
   "metadata": {},
   "source": [
    "#### Instanciando os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32175df5-b75f-4228-bcab-98977995b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_imgs_paths, train_labels, my_transform=my_trans)\n",
    "test_dataset = MyDataset(test_imgs_paths, test_labels, my_transform=my_trans_eval)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf209b-abca-4f9c-a3dd-66f543aa4854",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Criando um Dataloader\n",
    "- Agora, podemos criar um [dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) como fizemos nas últimas duas aulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213164a5-937b-446a-ad2a-dadf4ee81f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa362d3",
   "metadata": {},
   "source": [
    "## (Re)criando uma CNN\n",
    "- Vamos criar uma CNN para classificar os felinos\n",
    "- Para facilitar o projeto, vamos criar uma função que calcula o tamanho da saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=2), # 109\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # 54\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=7, stride=2), # 24\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=4)) # 6\n",
    "        self.fc = nn.Linear(576, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)        \n",
    "        out = self.layer2(out)\n",
    "        # Fazendo a operação de flatten        \n",
    "        out = out.reshape(out.size(0), -1)        \n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7bdf2",
   "metadata": {},
   "source": [
    "- Agora podemos instanciar o nosso modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc94922",
   "metadata": {},
   "source": [
    "- Agora podemos determinar nossa função de custo e otimizador\n",
    "- Para esse notebook vamos aproveitar o que já fizemos no anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a2ad1",
   "metadata": {},
   "source": [
    "- Agora vamos fazer nosso loop de treinamento\n",
    "- Agora, vamos mandar nosso modelo para GPU se ela estiver disponível\n",
    "- **Novo**: vamos criar nosso pipeline de checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Movendo o modelo para o device alvo\n",
    "model.to(device)\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    loss_epoch, cnt = 0, 0\n",
    "    for k, (batch_images, batch_labels, id_img) in enumerate(train_dataloader):  \n",
    "        \n",
    "        # Aplicando um flatten na imagem e movendo ela para o device alvo\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Fazendo a forward pass\n",
    "        # observe que o modelo é agnóstico ao batch size\n",
    "        outputs = model(batch_images)\n",
    "        loss = loss_func(outputs, batch_labels)\n",
    "        loss_epoch += loss.item()\n",
    "        \n",
    "        # Fazendo a otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        cnt += 1\n",
    "        \n",
    "    loss_epoch = loss_epoch/cnt\n",
    "        \n",
    "    # Salvando o checkpoint da última época\n",
    "    checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_func,\n",
    "            'loss_val': loss_epoch\n",
    "    }        \n",
    "    torch.save(checkpoint, \"last_checkpoint.pth\")\n",
    "\n",
    "    # Salvando a mellhor execução\n",
    "    nb = \"No\"\n",
    "    if loss_epoch < best_loss:\n",
    "        nb = \"Yes\"\n",
    "        best_loss = loss_epoch\n",
    "        torch.save(checkpoint, \"best_checkpoint.pth\")\n",
    "        \n",
    "    \n",
    "    print (f\"- Epoch [{epoch+1}/{num_epochs}] | Loss: {loss_epoch:.4f} | New best? {nb}\")                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf3356",
   "metadata": {},
   "source": [
    "### Fazendo inferência no conjunto de teste\n",
    "- A inferência é basicamente igual a do notebook anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdaec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for images, labels, img_id in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
