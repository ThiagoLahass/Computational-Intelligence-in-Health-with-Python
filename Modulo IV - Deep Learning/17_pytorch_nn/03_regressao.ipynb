{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e35cf99-bd0d-4814-bd41-6af7897e9b3d",
   "metadata": {},
   "source": [
    "# Iniciando com os submódulos de otimização e rede neural com uma regressão simples\n",
    "- O título do notebook já diz tudo\n",
    "- A ideia é abordamos dois submódulos muito importantes:    \n",
    "    - `torch.nn`: https://pytorch.org/docs/stable/nn.html\n",
    "        - Contém as funções básicas para construção de redes neurais\n",
    "    - `torch.optim`: https://pytorch.org/docs/stable/optim.html\n",
    "        - Contém os algortimos mais utilizados e conhecidos para otimização de redes neurais (ex: SGD)\n",
    "        \n",
    "- A ideia não é introduzir todas as funções existentes dentro dos módulos  \n",
    "- Vamos ir aprendendo aos poucos, conforme vamos introduzindo alguns exemplos e conceitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f37963-2924-4361-90cb-7cae968ff9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624cb96-ecd9-418b-b357-f4e349da6906",
   "metadata": {},
   "source": [
    "## Carregando uma base de dados\n",
    "- Como de costume, vamos carregar uma base do `sklearn` apenas para treinarmos conceitos da Pytorch\n",
    "    - Obs: Pytorch também tem um submodulo de datasets, mas vamos utilizar em breve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a423b75-ae37-4656-ba1b-323a037e6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "califa = datasets.fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(califa[\"data\"], califa[\"target\"], test_size=0.25, random_state=8)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39cf17-5e52-426b-ae2c-fce0dcebe9d5",
   "metadata": {},
   "source": [
    "- Aplicando uma normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9bfe1-ec0c-4bf1-bdbe-0c462ec45081",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc3d90-e39e-47be-850c-a279047d95ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definindo uma regressão linear\n",
    "\n",
    "## Definindo o modelo\n",
    "- O primeiro passo é definir o modelo da regressão linear\n",
    "- Dentro do submódulo `torch.nn`, um neurônio perceptron é definido como `torch.nn.Linear()`\n",
    "    - Basicamente aplica uma transformação linear $y = xA^{\\top}+b$\n",
    "    - [Documentação](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72159f-49f3-4f3b-a885-7b5fe5518d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "output_size = 1\n",
    "reg_model = nn.Linear(input_size, output_size)\n",
    "reg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a948e8-3cae-4473-b4fa-6c38d1769673",
   "metadata": {},
   "source": [
    "### Definindo a função de perda e o algoritmo de otimização\n",
    "- O segundo passo é definir qual a função de perda vamos utilizar e qual o algoritmo de otimização\n",
    "- O módulo `torch.nn` fornece um gama de funções de perda\n",
    "    - [Documentação](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "    - Para esse exemplo, vamos usar a MSE: `torch.nn.MSELoss()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88652d80-2c21-4c22-9449-9cdee037f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27ba52-131e-4871-966f-c9098e098ac1",
   "metadata": {},
   "source": [
    "- Agora, precisamos definir um método de otimização\n",
    "- Como já sabemos, métodos baseados em gradiente são o padrão para otimizarmos algoritmos de machine learning\n",
    "- Neste exemplo, vamos utilizar o SGD, que já vimos a ideia por trás do algoritmo\n",
    "    - [Documentação](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)\n",
    "    - `torch.optim.SGD`\n",
    "- Para esse método, temos que passar, obrigatoriamente, quais são os parâmetros a serem otimizados e a taxa de aprendizado\n",
    "    - Os demais parâmetros são opcionais\n",
    "    \n",
    "- Para obtermos todos os parametros treinaveis de um modelo (também conhecido como pesos), podemos chamar o método `parameters()` do modelo já declarado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd14ef-bf91-4a6d-8c0d-a1bdd6f60995",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(reg_model.parameters(), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc9ea7-ec34-4828-8308-b2c36711db21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loop de treinamento\n",
    "- Agora que já definimos todos os passos, precisamos fazer nosso loop de treinamento\n",
    "- Lembre-se que o gradiente descendente é um algoritmo iterativo, precisamos atualizar os pesos a cada época\n",
    "- Para isso, vamos usar a diferenciação automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb1708-7f28-48e1-a5cd-799169005417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    inputs = torch.from_numpy(X_train_norm).float()\n",
    "    targets = torch.from_numpy(y_train).float()\n",
    "    \n",
    "    # Fazendo a forwardpass\n",
    "    outputs = reg_model(inputs)\n",
    "    error = loss_func(outputs.flatten(), targets)\n",
    "    \n",
    "    # Agora aplicando a backward pass e fazendo a otimização\n",
    "    optimizer.zero_grad()\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f\"Epoch [{epoch+1}/{num_epochs}], MSE: {error.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206855c8-26ee-4b6a-9b76-6a89c5561b17",
   "metadata": {},
   "source": [
    "## Salvando e carregando um modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dddc1c-a7e5-43c1-9010-fbd7dbabac4c",
   "metadata": {},
   "source": [
    "- Salvando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390db3ad-ba09-4b0e-8599-2bdf9b9c2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reg_model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f6c13-f141-47f4-879f-d5539b866ce9",
   "metadata": {},
   "source": [
    "- Carregando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4b555-37f9-44aa-9009-a5c997e32a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = torch.load(\"model.pth\")\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555b253-cc8a-4d85-a9fe-56b82024dade",
   "metadata": {},
   "source": [
    "### Fazendo uma predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1f55e-df1e-4b93-9d7d-9a67aa75cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = my_model(torch.from_numpy(X_test_norm).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e51d03-54c8-48fd-a756-712774afa21f",
   "metadata": {},
   "source": [
    "- Calculando o erro no conjunto de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa80db-6065-407b-badd-6b24a37f0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(preds.flatten(), torch.from_numpy(y_test).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0fe0a-2a96-4d59-8d03-6d1a21b76c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(preds.numpy()[0:100], marker=\"o\", linestyle=\"dotted\", label=\"Predito\", color=\"b\")\n",
    "ax.plot(y_test[0:100], marker=\"o\", linestyle=\"dotted\", label=\"Real\", color=\"g\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de15015-4ee6-4091-9aa5-7b49e26427e4",
   "metadata": {},
   "source": [
    "___\n",
    "# Exercícios\n",
    "1. Aplique a técnica de minibatch no loop de treinamento feito acima\n",
    "2. Escolha uma base de dados de classificação e aplique uma regressão logística utilizando o que foi aprendido nesse notebook\n",
    "    - Dicas:\n",
    "        - Você precisa adicionar uma sigmoid no final da regressão linear\n",
    "        - Você precisa alterar a função de perda para lidar com problema de classificação\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
